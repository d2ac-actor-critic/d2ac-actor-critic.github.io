<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
<head>
    <meta charset="UTF-8">
    <title>D2AC: Diffusion Actor meets Distributional Critic</title>
    <meta name="description"
          content="We develop a new model-free reinforcement learning (RL) algorithm: D2AC. 
          Motivated by recent advances in model-based RL, we make two adjustments to the typical actor critic RL pipeline. 
          First, we learn distributional critics with a novel fusion of distributional RL and clipped double Q-learning. 
          Second, we use a diffusion model to parameterize the policy, and derive an efficient method for aligning the diffusion process with policy improvement. 
          These changes are highly effective, resulting in highly performant model-free policies on a benchmark of eighteen hard RL tasks, 
          including Humanoid, Dog, and Shadow Hand domains, spanning both dense-reward and goal-conditioned RL scenarios. D2AC significantly closes the performance gap between model-free and model-based RL methods, 
          while being at least four times faster to run than model-based counterparts.">

    <meta name="keywords" content="Actor Critic, Reinforcement Learning, Diffusion Models">
    <meta property="og:title" content="D2AC: Diffusion Actor meets Distributional Critic">
    <link rel="stylesheet" href="./style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>

</head>
<body>
<article>
    <section id="frontmatter">
        <h1>D2 Actor Critic:</h1>
        <h1 style="margin-top: 20px;">Diffusion Actor Meets Distributional Critic</h1>
        <h2 style="margin-top: 30px;">Abstract</h2>
    </section>
    <p>
    We develop a new model-free reinforcement learning (RL) algorithm: <b>D2AC</b>. Motivated by recent advances in model-based RL, we make two adjustments to the typical actor critic RL pipeline. 
    First, we learn distributional critics with a novel fusion of distributional RL and clipped double Q-learning. 
    Second, we use a diffusion model to parameterize the policy, and derive an efficient method for aligning the diffusion process with policy improvement. 
    These changes are highly effective, resulting in highly performant model-free policies on a benchmark of eighteen hard RL tasks, 
    including Humanoid, Dog, and Shadow Hand domains, spanning both dense-reward and goal-conditioned RL scenarios. 
    D2AC significantly closes the performance gap between model-free and model-based RL methods, 
    while being at least four times faster to run than model-based counterparts.
    </p>

    <h2>What is an actor critic method?</h2>
    <p>
        <br>Actor Critic is a class of methods for solving reinforcement learning problems, where an agent interacts with an environment to maximize the cumulative reward signals.
        <br>In Actor Critic methods, there are two components: (i) an actor that takes actions in an environment; (ii) a critic that estimates the potential return of being in a specific state and taking an action.
    </p>

    <p>
        <img src="./media/d2ac-web-viz.png" width="95%" height="auto" alt="d2ac-overall" class="center">
    </p>

</article>
</body>
</html>
