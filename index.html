<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
<head>
    <meta charset="UTF-8">
    <title>D2AC: Diffusion Actor meets Distributional Critic</title>
    <meta name="description"
          content="We develop a new model-free reinforcement learning (RL) algorithm: D2AC. 
          Motivated by recent advances in model-based RL, we make two adjustments to the typical actor critic RL pipeline. 
          First, we learn distributional critics with a novel fusion of distributional RL and clipped double Q-learning. 
          Second, we use a diffusion model to parameterize the policy, and derive an efficient method for aligning the diffusion process with policy improvement. 
          These changes are highly effective, resulting in highly performant model-free policies on a benchmark of eighteen hard RL tasks, 
          including Humanoid, Dog, and Shadow Hand domains, spanning both dense-reward and goal-conditioned RL scenarios. D2AC significantly closes the performance gap between model-free and model-based RL methods, 
          while being at least four times faster to run than model-based counterparts.">

    <meta name="keywords" content="Actor Critic, Reinforcement Learning, Diffusion Models">
    <meta property="og:title" content="D2AC: Diffusion Actor meets Distributional Critic">
    <link rel="stylesheet" href="./style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>

</head>
<body>
<article>
    <section id="frontmatter">
        <h1>D2 Actor Critic:</h1>
        <h1 style="margin-top: 20px;">Diffusion Actor Meets Distributional Critic</h1>
        <h2 style="margin-top: 30px;"></h2>
    </section>
    <p>
    We develop a new model-free reinforcement learning (RL) algorithm: <b>D2AC</b>. Motivated by recent advances in model-based RL, we make two adjustments to the typical actor critic RL pipeline. 
    First, we learn distributional critics with a novel fusion of distributional RL and clipped double Q-learning. 
    Second, we use a diffusion model to parameterize the policy, and derive an efficient method for aligning the diffusion process with policy improvement. 
    These changes are highly effective, resulting in highly performant model-free policies on a benchmark of eighteen hard RL tasks, 
    including Humanoid, Dog, and Shadow Hand domains, spanning both dense-reward and goal-conditioned RL scenarios. 
    D2AC significantly closes the performance gap between model-free and model-based RL methods, 
    while being at least four times faster to run than model-based counterparts.
    </p>

    <p>
        <img src="./media/d2ac-web-viz.png" width="95%" height="auto" alt="d2ac-overall">
    </p>

    <h2>What is an Actor-Critic method?</h2>
    <p>
        <br>Actor Critic is a class of methods for solving reinforcement learning problems, where an agent interacts with an environment to maximize the cumulative reward signals.
        <br>In Actor Critic methods, there are only two components: (i) an actor that takes actions in an environment; (ii) a critic that estimates the potential return of being in a specific state and taking an action.
    </p>

    <h2>What is Model-Predictive Control?</h2>
    <p>
        <br>Model-predictive control (MPC) learns a model of the environment and uses it to plan actions. MPC is sample-efficient but computationally expensive.
        <br>MPC has two intriguing properties that we aim to capture in <i>model-free</i> D2AC:
        <br>(i) MPC tends to model a distribution rather than a point estimate of future returns;
        <br>(ii) MPC uses an iterative refinement process on action proposals, akin to denoising diffusion for policy improvement.
    </p>

    <h2>What is D2 Actor Critic?</h2>
    <p>
        <br>D2AC employs distributional critics to model return distributions, and additionally applies the clipped double Q-learning technique to make Q-function estimates more accurate.
        <br>D2AC uses a denoising diffusion model for the policy network, and derives a novel and simplified policy improvement objective to supervise each step of the denoising process so that action proposals can be iteratively refined at inference.
    </p>

    <h2>Video of D2AC Policies</h2>
    <p>
        <br>No model-based planning is performed at either training or inference.
    </p>
    <p>
    <video autoplay loop muted controls width="31%" height="auto">
        <source src="media/dog_run_3_video.webm">
    </video>

    <video autoplay loop muted controls width="31%" height="auto">
        <source src="media/humanoid-run-video-v3.webm">
    </video>

    <video autoplay loop muted controls width="31%" height="auto">
        <source src="media/quadruped_v2.webm">
    </video>
    </p>

    <p>
    <video autoplay loop muted controls width="31%" height="auto">
        <source src="media/push.webm">
    </video>
    <video autoplay loop muted controls width="31%" height="auto">
        <source src="media/pick_and_place.webm">
    </video>
    <video autoplay loop muted controls width="31%" height="auto">
        <source src="media/slide.webm">
    </video>
    </p>

    <p>
    <video autoplay loop muted controls width="31%" height="auto">
        <source src="media/manipulate_pen_rotate.webm">
    </video>
    <video autoplay loop muted controls width="31%" height="auto">
        <source src="media/manipulate_blockz.webm">
    </video>
    <video autoplay loop muted controls width="31%" height="auto">
        <source src="media/manipulate_egg_rotate.webm">
    </video>
    </p>
    
</article>
</body>
</html>
